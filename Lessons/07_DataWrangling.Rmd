---
title: "7: Data Wrangling"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## LESSON OBJECTIVES
1. Describe the usefulness of data wrangling and its place in the data pipeline
2. Wrangle datasets with dplyr functions
3. Apply data wrangling skills to a real-world example dataset

## OPENING DISCUSSION

After we've completed basic data exploration on a dataset, what step comes next? How does this help us to ask and answer questions about datasets?

> ANSWER: Data wrangling; it helps us get the data in a format that is useful for our analysis.

## SET UP YOUR DATA ANALYSIS SESSION

In assignment 3, you explored the North Temperate Lakes Long-Term Ecological Research Station data for physical and chemical data. What did you learn about this dataset in your assignment?

> There's a relationship between temp. and depth. They also measured oxygen content, etc.

We will continue working with this dataset today. 

```{r}
getwd()
library(tidyverse)
NTL.phys.data <- read.csv("./Data/Raw/NTL-LTER_Lake_ChemistryPhysics_Raw.csv")

head(NTL.phys.data)
colnames(NTL.phys.data)
summary(NTL.phys.data)
dim(NTL.phys.data)

```

## DATA WRANGLING

Data wrangling takes data exploration one step further: it allows you to process data in ways that are useful for you. An important part of data wrangling is creating tidy datasets, with the following rules: 

1. Each variable has its own column
2. Each observation has its own row
3. Each value has its own cell

What is the best way to wrangle data? There are multiple ways to arrive at a specific outcome in R, and we will illustrate some of those approaches. Your goal should be to write the simplest and most elegant code that will get you to your desired outcome. However, there is sometimes a trade-off of the opportunity cost to learn a new formulation of code and the time it takes to write complex code that you already know. Remember that the best code is one that is easy to understand for yourself and your collaborators. Remember to comment your code, use informative names for variables and functions, and use reproducible methods to arrive at your output.

## WRANGLING IN R: DPLYR

`dplyr` is a package in R that includes functions for data manipulation (i.e., data wrangling or data munging). `dplyr` is included in the tidyverse package, so you should already have it installed on your machine. The functions act as verbs for data wrangling processes. For more information, run this line of code:

```{r, results = "hide"}
vignette("dplyr")

# d stands for dataset, plyr is a reference to actual plyers, like the tool 
# vignette is helpful in explaining what it is. It's an intro to dplyr
```


### Filter

Filtering allows us to choose certain rows (observations) in our dataset.

A few relevant commands: 
`==`
> equal
`!=`
> is not
`<`
`<=`
`>`
`>=`
`&`
`|`
> or

```{r}
class(NTL.phys.data$lakeid)
class(NTL.phys.data$depth)

# matrix filtering
NTL.phys.data.surface1 <- NTL.phys.data[NTL.phys.data$depth == 0,]

# This ^ is creating a subset of the data, showing us observations at a depth of only 0, while keeping the row names from the original dataset

# dplyr filtering
NTL.phys.data.surface2 <- filter(NTL.phys.data, depth == 0)
NTL.phys.data.surface3 <- filter(NTL.phys.data, depth < 0.25)

# This ^ does the same thing as the matrix filtering, except it stores the rows as new row names

# Did the methods arrive at the same result?
head(NTL.phys.data.surface1)
dim(NTL.phys.data.surface1)
head(NTL.phys.data.surface2)
dim(NTL.phys.data.surface2)
head(NTL.phys.data.surface3)
dim(NTL.phys.data.surface3)

# Choose multiple conditions to filter
summary(NTL.phys.data$lakename)
NTL.phys.data.PeterPaul1 <- filter(NTL.phys.data, lakename == "Paul Lake" | lakename == "Peter Lake")
NTL.phys.data.PeterPaul2 <- filter(NTL.phys.data, lakename != "Central Long Lake" & 
                                     lakename != "Crampton Lake" & lakename != "East Long Lake" &
                                     lakename != "Hummingbird Lake" & lakename != "Tuesday Lake" &
                                     lakename != "Ward Lake" & lakename != "West Long Lake")
NTL.phys.data.PeterPaul3 <- filter(NTL.phys.data, lakename %in% c("Paul Lake", "Peter Lake"))

# Use | to include things, use & to exclude things
# PeterPaul3 uses %in%, which means 'include' the 

# Choose a range of conditions of a numeric or integer variable
summary(NTL.phys.data$daynum)
NTL.phys.data.JunethruOctober1 <- filter(NTL.phys.data, daynum > 151 & daynum < 305)
NTL.phys.data.JunethruOctober2 <- filter(NTL.phys.data, daynum > 151, daynum < 305)
NTL.phys.data.JunethruOctober3 <- filter(NTL.phys.data, daynum >= 152 & daynum <= 304)
NTL.phys.data.JunethruOctober4 <- filter(NTL.phys.data, daynum %in% c(152:304))

# These are various ways to filter a variable (in this case it's day #)

# Exercise: 
# filter NTL.phys.data for the year 1999
# what code do you need to use, based on the class of the variable? ANSWER: character or factor must be put in quotes
class(NTL.phys.data$year4)

NTL.phys.data.1999 <- filter(NTL.phys.data, year4 == 1999)

# Exercise: 
# filter NTL.phys.data for Tuesday Lake from 1990 through 1999.
NTL.phys.data.Tues90thru99 <- filter(NTL.phys.data, year4 >= 1990 & year4 <= 1999, 
                                         lakename == "Tuesday Lake")

View(NTL.phys.data.Tues90thru99)
```
Question: Why don't we filter using row numbers?

> ANSWER: It's not easy to understand, and it's not so efficient to look through the rows to find what you want. Plus, if data is added to the raw file, it wouldn't be included 

### Arrange

Arranging allows us to change the order of rows in our dataset. By default, the arrange function will arrange rows in ascending order.

```{r}
NTL.phys.data.depth.ascending <- arrange(NTL.phys.data, depth)
NTL.phys.data.depth.descending <- arrange(NTL.phys.data, desc(depth))

# This arranges our data just like "filter" on excel arranges a variable (column) in ascending/descending order

# Exercise: 
# Arrange NTL.phys.data by temperature, in descending order. 
# Which dates, lakes, and depths have the highest temperatures? ANSWER: 7/16/98, East Long Lake, 0.5
NTL.phys.data.temp.descending <- arrange(NTL.phys.data, desc(temperature_C))


```
### Select

Selecting allows us to choose certain columns (variables) in our dataset.

```{r}
NTL.phys.data.temps <- select(NTL.phys.data, lakename, sampledate:temperature_C)
# We're creating a table that shows the lakename column, and the columns between and including date and temp
# Only use commas and colons to select columns; we can't use & or other functions from filter (i.e. it has more limited syntax than filter does)
```
### Mutate

Mutating allows us to add new columns that are functions of existing columns. Operations include addition, subtraction, multiplication, division, log, and other functions.

```{r}

NTL.phys.data.temps <- mutate(NTL.phys.data.temps, temperature_F = (temperature_C*9/5) + 32)

# This code is adding a column named temperature_F, and it's equal to the equation to convert C to F, using the value from column temp_C

```
### Pipes

Sometimes we will want to perform multiple commands on a single dataset on our way to creating a processed dataset. We could do this in a series of subsequent commands or create a function. However, there is another method to do this that looks cleaner and is easier to read. This method is called a pipe. We designate a pipe with `%>%`. A good way to think about the function of a pipe is with the word "then." 

Let's say we want to take our raw dataset (NTL.phys.data), *then* filter the data for Peter and Paul lakes, *then* select temperature and observation information, and *then* add a column for temperature in Fahrenheit: 

```{r}
NTL.phys.data.processed <- 
  NTL.phys.data %>%
  filter(lakename == "Paul Lake" | lakename == "Peter Lake") %>%
  select(lakename, sampledate:temperature_C) %>%
  mutate(temperature_F = (temperature_C*9/5) + 32)

# There's no need to include the dataframe name for each command within the pipes command (reiterated directly below)
  
```

Notice that we did not place the dataset name inside the wrangling function but rather at the beginning.

### Saving processed datasets

```{r}
write.csv(NTL.phys.data.PeterPaul1, row.names = FALSE, file = "./Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv")

# Row names = FALSE excludes the first column, which is just a count of the rows
# one . before /Data is going down one folder

```

## CLOSING DISCUSSION
How did data wrangling help us to generate a processed dataset? How does this impact our ability to analyze and answer questions about our data?

> ANSWER: 